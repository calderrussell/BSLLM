{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Building a Brandon Sanderson GPT\n",
        "\n",
        "Notebook made with [Zero To Hero](https://karpathy.ai/zero-to-hero.html) video on GPT."
      ],
      "metadata": {
        "id": "wJpXpmjEYC_T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5hjCcLDr2WC",
        "outputId": "d28a405c-6860-4848-f8f5-8da57600097c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-03 18:43:44--  https://raw.githubusercontent.com/calderrussell/BSLLM/main/data_new.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6261732 (6.0M) [application/octet-stream]\n",
            "Saving to: ‘data_new.txt.2’\n",
            "\n",
            "data_new.txt.2      100%[===================>]   5.97M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2025-07-03 18:43:44 (104 MB/s) - ‘data_new.txt.2’ saved [6261732/6261732]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
        "!wget https://raw.githubusercontent.com/calderrussell/BSLLM/main/data_new.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read it in to inspect it\n",
        "with open('data_new.txt', 'r', encoding='mac_roman') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "O6medjfRsLD9"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"length of dataset in characters: \", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xWI_VyAsN8F",
        "outputId": "3781357a-707b-4602-846d-352356436b8a"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  6261732\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's look at the first 1000 characters\n",
        "print(text[:5000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c5V0FvqseE0",
        "outputId": "8de0d2c1-572a-45d2-8ae4-de4b01190ac5"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "FOR JOSHUA BILMES \n",
            "Who is never afraid to tell me what is wrong with a book, then ﬁght for that same book no matter who else gives up on it. \n",
            "\n",
            "ACKNOWLEDGMENTS \n",
            "I ﬁrst pitched the idea of later-era Mistborn novels to my editor back in 2006, I believe. It had long been my plan for Scadrial, the planet these books take place upon. I wanted to move away from the idea of fantasy worlds as static places, where millennia would pass and technology would never change. The plan then was for a second epic trilogy set in an urban era, and a third trilogy set in a futuristic era— with Allomancy, Feruchemy, and Hemalurgy being the common threads that tied them together. \n",
            "This book isn’t part of that second trilogy. It’s a side deviation, something exciting that grew quite unexpectedly out of my planning for where the world would go. The point of telling you all of this, however, is to explain that it would be impossible to list all of the people who have helped me along the years. Instead, the best I can do is list some of the wonderful people who helped me with this speciﬁc book. \n",
            "Alpha readers included, as always, my agent, Joshua Bilmes, and my editor, Moshe Feder. This book is dedicated to Joshua, actually. Professionally, he’s believed in my work longer than anyone outside my writing group. He has been a wonderful resource and a good friend. \n",
            "Other alphas were my writing group: Ethan Skarstedt, Dan Wells, Alan & Jeanette Layton, Kaylynn ZoBell, Karen Ahlstrom, Ben & Danielle Olsen, Jordan Sanderson (kind of), and Kathleen Dorsey. Finally, of course, there’s the Inseparable Peter Ahlstrom, my assistant and friend, who does all kinds of important things for my writing and doesn’t get nearly enough thanks for it. \n",
            "At Tor Books, thanks go to Irene Gallo, Justin Golenbock, Terry McGarry, and many others I couldn’t possibly name—everyone from Tom Doherty to the sales force. Thank you all for your excellent work. Once again, I feel the need to give a special thanks to Paul Stevens, who goes above and beyond what I could reasonably expect to give aid and explanations. \n",
            "Beta readers included Je. Creer and Dominique Nolan. A special thanks to Dom for being a resource in regards to weaponry and guns. If you ever need anything shot properly, he’s the one to call. \n",
            "Note the lovely cover by Chris McGrath, whom I asked for speciﬁcally because of his work on the Mistborn paperback covers. Both Ben McSweeney and Isaac Stewart returned to provide interior art for this book, as their work on The Way of Kings was just plain awesome. They’ve continued in their awesomeness. Ben also provided equally awesome illustrations for the recently released Mistborn RPG from Crafty Games. Check it out at crafty-games.com, especially if you’re interested in Kelsier’s origin story. \n",
            "Last of all I’d like to once again thank Emily, my wonderful wife, for her support, commentary, and love. \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "CONTENTS \n",
            "Title Page \n",
            "Dedication \n",
            "Acknowledgments \n",
            "Maps \n",
            "\n",
            "Prologue \n",
            "Chapter 1 \n",
            "Chapter 2 \n",
            "Chapter 3 \n",
            "Chapter 4 \n",
            "Chapter 5 \n",
            "Chapter 6 \n",
            "Chapter 7 \n",
            "Chapter 8 \n",
            "Chapter 9 \n",
            "Chapter 10 \n",
            "Chapter 11 \n",
            "Chapter 12 \n",
            "Chapter 13 \n",
            "Chapter 14 \n",
            "Chapter 15 \n",
            "Chapter 16 \n",
            "Chapter 17 \n",
            "Chapter 18 \n",
            "Chapter 19 \n",
            "Chapter 20 \n",
            "Epilogue \n",
            "\n",
            "ARS Arcanum \n",
            "Tor Books by Brandon Sanderson \n",
            "Copyright \n",
            "\n",
            "\n",
            "PROLOGUE \n",
            "\n",
            "Wax crept along the ragged fence in a crouch, his boots scraping the dry ground. He held his Sterrion 36 up by his head, the long, silvery barrel dusted with red clay. The revolver was nothing fancy to look at, though the six-shot cylinder was machined with such care in the steel-alloy frame that there was no play in its movement. There was no gleam to the metal or exotic material on the grip. But it ﬁt his hand like it was meant to be there. \n",
            "The waist-high fence was ﬂimsy, the wood grayed with time, held together with fraying lengths of rope. It smelled of age. Even the worms had given up on this wood long ago. \n",
            "Wax peeked up over the knotted boards, scanning the empty town. Blue lines hovered in his vision, extending from his chest to point at nearby sources of metal, a result of his Allomancy. Burning steel did that; it let him see the location of sources of metal, then Push against them if he wanted. His weight against the weight of the item. If it was heavier, he was pushed back. If he was heavier, it was pushed forward. \n",
            "In this case, however, he didn’t Push. He just watched the lines to see if any of the metal was moving. None of it was. Nails holding together buildings, spent shell casings lying scattered in the dust, or horseshoes piled at the silent smithy—all were as motionless as the old hand pump planted in the ground to his right. \n",
            "Wary, he too remained still. Steel continued to burn comfortably in his stomach, and so—as a precaution—he gently Pushed outward from himself in all directions. It was a trick he’d mastered a few years back; he didn’t Push on any speciﬁc metal objects, but created a kind of defensive bubble around himself. Any metal that came streaking in his dire\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "chars = chars[:-15]\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e-Rbyr8sfM8",
        "outputId": "e068d256-b9b0-4ba1-8a16-c1fcc108d699"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !\"#$&'()*,-./0123456789:;=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]_abcdefghijklmnopqrstuvwxyz\n",
            "87\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "filtered_text = ''.join([c for c in text if c in stoi])\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "print(encode(\"hii there\"))\n",
        "print(decode(encode(\"hii there\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yw1LKNCgwjj1",
        "outputId": "98989328-edc2-4829-d0a0-ff0add667e90"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[68, 69, 69, 1, 80, 68, 65, 78, 65]\n",
            "hii there\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
        "import torch # we use PyTorch: https://pytorch.org\n",
        "data = torch.tensor(encode(filtered_text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJb0OXPwzvqg",
        "outputId": "32641b09-8b52-4fb8-9963-1adf7ea5bf44"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([6035142]) torch.int64\n",
            "tensor([ 0,  0, 36, 45, 48,  1, 40, 45, 49, 38, 51, 31,  1, 32, 39, 42, 43, 35,\n",
            "        49,  1,  0, 53, 68, 75,  1, 69, 79,  1, 74, 65, 82, 65, 78,  1, 61, 66,\n",
            "        78, 61, 69, 64,  1, 80, 75,  1, 80, 65, 72, 72,  1, 73, 65,  1, 83, 68,\n",
            "        61, 80,  1, 69, 79,  1, 83, 78, 75, 74, 67,  1, 83, 69, 80, 68,  1, 61,\n",
            "         1, 62, 75, 75, 71, 11,  1, 80, 68, 65, 74,  1, 67, 68, 80,  1, 66, 75,\n",
            "        78,  1, 80, 68, 61, 80,  1, 79, 61, 73, 65,  1, 62, 75, 75, 71,  1, 74,\n",
            "        75,  1, 73, 61, 80, 80, 65, 78,  1, 83, 68, 75,  1, 65, 72, 79, 65,  1,\n",
            "        67, 69, 82, 65, 79,  1, 81, 76,  1, 75, 74,  1, 69, 80, 13,  1,  0,  0,\n",
            "        31, 33, 41, 44, 45, 53, 42, 35, 34, 37, 43, 35, 44, 50, 49,  1,  0, 39,\n",
            "         1, 78, 79, 80,  1, 76, 69, 80, 63, 68, 65, 64,  1, 80, 68, 65,  1, 69,\n",
            "        64, 65, 61,  1, 75, 66,  1, 72, 61, 80, 65, 78, 12, 65, 78, 61,  1, 43,\n",
            "        69, 79, 80, 62, 75, 78, 74,  1, 74, 75, 82, 65, 72, 79,  1, 80, 75,  1,\n",
            "        73, 85,  1, 65, 64, 69, 80, 75, 78,  1, 62, 61, 63, 71,  1, 69, 74,  1,\n",
            "        17, 15, 15, 21, 11,  1, 39,  1, 62, 65, 72, 69, 65, 82, 65, 13,  1, 39,\n",
            "        80,  1, 68, 61, 64,  1, 72, 75, 74, 67,  1, 62, 65, 65, 74,  1, 73, 85,\n",
            "         1, 76, 72, 61, 74,  1, 66, 75, 78,  1, 49, 63, 61, 64, 78, 69, 61, 72,\n",
            "        11,  1, 80, 68, 65,  1, 76, 72, 61, 74, 65, 80,  1, 80, 68, 65, 79, 65,\n",
            "         1, 62, 75, 75, 71, 79,  1, 80, 61, 71, 65,  1, 76, 72, 61, 63, 65,  1,\n",
            "        81, 76, 75, 74, 13,  1, 39,  1, 83, 61, 74, 80, 65, 64,  1, 80, 75,  1,\n",
            "        73, 75, 82, 65,  1, 61, 83, 61, 85,  1, 66, 78, 75, 73,  1, 80, 68, 65,\n",
            "         1, 69, 64, 65, 61,  1, 75, 66,  1, 66, 61, 74, 80, 61, 79, 85,  1, 83,\n",
            "        75, 78, 72, 64, 79,  1, 61, 79,  1, 79, 80, 61, 80, 69, 63,  1, 76, 72,\n",
            "        61, 63, 65, 79, 11,  1, 83, 68, 65, 78, 65,  1, 73, 69, 72, 72, 65, 74,\n",
            "        74, 69, 61,  1, 83, 75, 81, 72, 64,  1, 76, 61, 79, 79,  1, 61, 74, 64,\n",
            "         1, 80, 65, 63, 68, 74, 75, 72, 75, 67, 85,  1, 83, 75, 81, 72, 64,  1,\n",
            "        74, 65, 82, 65, 78,  1, 63, 68, 61, 74, 67, 65, 13,  1, 50, 68, 65,  1,\n",
            "        76, 72, 61, 74,  1, 80, 68, 65, 74,  1, 83, 61, 79,  1, 66, 75, 78,  1,\n",
            "        61,  1, 79, 65, 63, 75, 74, 64,  1, 65, 76, 69, 63,  1, 80, 78, 69, 72,\n",
            "        75, 67, 85,  1, 79, 65, 80,  1, 69, 74,  1, 61, 74,  1, 81, 78, 62, 61,\n",
            "        74,  1, 65, 78, 61, 11,  1, 61, 74, 64,  1, 61,  1, 80, 68, 69, 78, 64,\n",
            "         1, 80, 78, 69, 72, 75, 67, 85,  1, 79, 65, 80,  1, 69, 74,  1, 61,  1,\n",
            "        66, 81, 80, 81, 78, 69, 79, 80, 69, 63,  1, 65, 78, 61,  1, 83, 69, 80,\n",
            "        68,  1, 31, 72, 72, 75, 73, 61, 74, 63, 85, 11,  1, 36, 65, 78, 81, 63,\n",
            "        68, 65, 73, 85, 11,  1, 61, 74, 64,  1, 38, 65, 73, 61, 72, 81, 78, 67,\n",
            "        85,  1, 62, 65, 69, 74, 67,  1, 80, 68, 65,  1, 63, 75, 73, 73, 75, 74,\n",
            "         1, 80, 68, 78, 65, 61, 64, 79,  1, 80, 68, 61, 80,  1, 80, 69, 65, 64,\n",
            "         1, 80, 68, 65, 73,  1, 80, 75, 67, 65, 80, 68, 65, 78, 13,  1,  0, 50,\n",
            "        68, 69, 79,  1, 62, 75, 75, 71,  1, 69, 79, 74, 80,  1, 76, 61, 78, 80,\n",
            "         1, 75, 66,  1, 80, 68, 61, 80,  1, 79, 65, 63, 75, 74, 64,  1, 80, 78,\n",
            "        69, 72, 75, 67, 85, 13,  1, 39, 80, 79,  1, 61,  1, 79, 69, 64, 65,  1,\n",
            "        64, 65, 82, 69, 61, 80, 69, 75, 74, 11,  1, 79, 75, 73, 65, 80, 68, 69,\n",
            "        74, 67,  1, 65, 84, 63, 69, 80, 69, 74, 67,  1, 80, 68, 61, 80,  1, 67,\n",
            "        78, 65, 83,  1, 77, 81, 69, 80, 65,  1, 81, 74, 65, 84, 76, 65, 63, 80,\n",
            "        65, 64, 72, 85,  1, 75, 81, 80,  1, 75, 66,  1, 73, 85,  1, 76, 72, 61,\n",
            "        74, 74, 69, 74, 67,  1, 66, 75, 78,  1, 83, 68, 65, 78, 65,  1, 80, 68,\n",
            "        65,  1, 83, 75, 78, 72, 64,  1, 83, 75, 81, 72, 64,  1, 67, 75, 13,  1,\n",
            "        50, 68, 65,  1, 76, 75, 69, 74, 80,  1, 75, 66,  1, 80, 65, 72, 72, 69,\n",
            "        74, 67,  1, 85, 75, 81,  1, 61, 72, 72,  1, 75, 66,  1, 80, 68, 69, 79,\n",
            "        11,  1, 68, 75, 83, 65, 82, 65, 78, 11,  1, 69, 79,  1, 80, 75,  1, 65,\n",
            "        84, 76, 72, 61, 69, 74,  1, 80, 68, 61, 80,  1, 69, 80,  1, 83, 75, 81,\n",
            "        72, 64,  1, 62, 65,  1, 69, 73, 76, 75, 79, 79, 69, 62, 72, 65,  1, 80,\n",
            "        75,  1, 72, 69, 79, 80,  1, 61, 72, 72,  1, 75, 66,  1, 80, 68, 65,  1,\n",
            "        76, 65, 75, 76, 72, 65,  1, 83, 68, 75,  1, 68, 61, 82, 65,  1, 68, 65,\n",
            "        72, 76, 65, 64,  1, 73, 65,  1, 61, 72, 75, 74, 67,  1, 80, 68, 65,  1,\n",
            "        85, 65, 61, 78, 79, 13,  1, 39, 74, 79, 80, 65, 61, 64, 11,  1, 80, 68,\n",
            "        65,  1, 62, 65, 79, 80,  1, 39,  1, 63])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's now split up the data into train and validation sets\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "f_WIXqxz0lU5"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8\n",
        "train_data[:block_size+1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TD5Bj8Y6IAD4",
        "outputId": "d51f493a-f19f-4b03-f1fc-70cdd515853f"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0,  0, 36, 45, 48,  1, 40, 45, 49])"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"when input is {context} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HXDe8vGJCEn",
        "outputId": "afd0c43a-31ea-42e1-e7ea-32c08cc2d98d"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([0]) the target: 0\n",
            "when input is tensor([0, 0]) the target: 36\n",
            "when input is tensor([ 0,  0, 36]) the target: 45\n",
            "when input is tensor([ 0,  0, 36, 45]) the target: 48\n",
            "when input is tensor([ 0,  0, 36, 45, 48]) the target: 1\n",
            "when input is tensor([ 0,  0, 36, 45, 48,  1]) the target: 40\n",
            "when input is tensor([ 0,  0, 36, 45, 48,  1, 40]) the target: 45\n",
            "when input is tensor([ 0,  0, 36, 45, 48,  1, 40, 45]) the target: 49\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 4 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size): # batch dimension\n",
        "    for t in range(block_size): # time dimension\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b,t]\n",
        "        print(f\"when input is {context.tolist()} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3k1Czf7LuA9",
        "outputId": "4c0f5530-4f4e-4558-c825-8e79e2055563"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[83, 75, 78, 78, 69, 65, 79,  1],\n",
            "        [80, 68, 65, 64, 65, 66, 65, 61],\n",
            "        [69, 79, 65, 64,  1, 61, 74,  1],\n",
            "        [65, 13,  1, 31, 64, 61, 73,  1]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[75, 78, 78, 69, 65, 79,  1, 73],\n",
            "        [68, 65, 64, 65, 66, 65, 61, 80],\n",
            "        [79, 65, 64,  1, 61, 74,  1, 65],\n",
            "        [13,  1, 31, 64, 61, 73,  1, 38]])\n",
            "----\n",
            "when input is [83] the target: 75\n",
            "when input is [83, 75] the target: 78\n",
            "when input is [83, 75, 78] the target: 78\n",
            "when input is [83, 75, 78, 78] the target: 69\n",
            "when input is [83, 75, 78, 78, 69] the target: 65\n",
            "when input is [83, 75, 78, 78, 69, 65] the target: 79\n",
            "when input is [83, 75, 78, 78, 69, 65, 79] the target: 1\n",
            "when input is [83, 75, 78, 78, 69, 65, 79, 1] the target: 73\n",
            "when input is [80] the target: 68\n",
            "when input is [80, 68] the target: 65\n",
            "when input is [80, 68, 65] the target: 64\n",
            "when input is [80, 68, 65, 64] the target: 65\n",
            "when input is [80, 68, 65, 64, 65] the target: 66\n",
            "when input is [80, 68, 65, 64, 65, 66] the target: 65\n",
            "when input is [80, 68, 65, 64, 65, 66, 65] the target: 61\n",
            "when input is [80, 68, 65, 64, 65, 66, 65, 61] the target: 80\n",
            "when input is [69] the target: 79\n",
            "when input is [69, 79] the target: 65\n",
            "when input is [69, 79, 65] the target: 64\n",
            "when input is [69, 79, 65, 64] the target: 1\n",
            "when input is [69, 79, 65, 64, 1] the target: 61\n",
            "when input is [69, 79, 65, 64, 1, 61] the target: 74\n",
            "when input is [69, 79, 65, 64, 1, 61, 74] the target: 1\n",
            "when input is [69, 79, 65, 64, 1, 61, 74, 1] the target: 65\n",
            "when input is [65] the target: 13\n",
            "when input is [65, 13] the target: 1\n",
            "when input is [65, 13, 1] the target: 31\n",
            "when input is [65, 13, 1, 31] the target: 64\n",
            "when input is [65, 13, 1, 31, 64] the target: 61\n",
            "when input is [65, 13, 1, 31, 64, 61] the target: 73\n",
            "when input is [65, 13, 1, 31, 64, 61, 73] the target: 1\n",
            "when input is [65, 13, 1, 31, 64, 61, 73, 1] the target: 38\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(xb) # our input to the transformer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpyyAeIzQjlO",
        "outputId": "9cd409d5-7a65-45a2-e288-c400545f766d"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[83, 75, 78, 78, 69, 65, 79,  1],\n",
            "        [80, 68, 65, 64, 65, 66, 65, 61],\n",
            "        [69, 79, 65, 64,  1, 61, 74,  1],\n",
            "        [65, 13,  1, 31, 64, 61, 73,  1]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nql_1ER53oCf",
        "outputId": "cba250df-26f1-4cf3-ef78-34025475a550"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 87])\n",
            "tensor(5.2653, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "0(=2UQul,1'O>k[c/RpmlFTjfW4$-RWr\"\\ kL[4T_X?-UgAIP1;;e2q*d, CVNj.XBF*(=T?;C?eG*(0iIt\n",
            "\\38LeGpSDhvOOK0b\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "eTyJ8qAaDdiF"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "for steps in range(100): # increase number of steps for good results...\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hs4kI8YdEkQj",
        "outputId": "276d932b-d5c3-4ca7-ebb4-68885c257f11"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.808799743652344\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcVIDWAZEtjN",
        "outputId": "7a5d149c-e473-4ca9-9bec-80ab7d82e3be"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "bZgqYLgTZ?qT?jn'$$cJo7rSqkX2g]'7XPrQe1/*1:&VhFQZa1T>=Xn9ww[4;(=/RS$?-F\"POD3A42'3j)=KfWXZ$(QYMOB6ZM5iiH4O-R-#vcL(BLkL>O\n",
            "ax1a8uYUK/)i-#R-jun\n",
            "(=q*VSTru?ffkqF(LAsZ\n",
            "?/I8Kg7,A47\"/F26_uF,1!J-,&!0ryhZe=(Y==K?J-Ak550G5iq19fto:0:@s#\n",
            "lCpEJCll) \"ne6bUN1/=v5BeV#>Pp$mD4\\a vQzy-#msFx7&4to!M\\BkX;&Ci.!\\o7gGeacoyh_(dqscm@*kaNqY,YH#mmM/I2bI1xKWX)\"IDjG9!Y,&=L6Z?MeEe(nkAHTRQV!2Ux?R .WNBk_D/exV8LBVvq*L*Oy7Q:NGe&*uI]xO:I&L=D7g!XK 3uc9pxr-.?@PS9Bwc\"Vxn1nz/zBxy U*Nq*sb0nk[8 4Vz7x2idY24G4Tr:Ur*9Q6(h>2F*9$_M]4[/R;:;E=>dBx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The mathematical trick in self-attention"
      ],
      "metadata": {
        "id": "XinV8nmAnmKN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3, 3))\n",
        "a = a / torch.sum(a, 1, keepdim=True)\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "c = a @ b\n",
        "print('a=')\n",
        "print(a)\n",
        "print('--')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('--')\n",
        "print('c=')\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tukiH-NbRBhA",
        "outputId": "3ac0b585-071f-4874-bd35-e6a241629b97"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "--\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "--\n",
            "c=\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# consider the following toy example:\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,2 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hs_E24uRE8kr",
        "outputId": "e083e5d5-684c-487e-acac-c379e393a35a"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
        "xbow = torch.zeros((B,T,C))\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        xprev = x[b,:t+1] # (t,C)\n",
        "        xbow[b,t] = torch.mean(xprev, 0)\n"
      ],
      "metadata": {
        "id": "86NuXX0fn7ps"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# version 2: using matrix multiply for a weighted aggregation\n",
        "wei = torch.tril(torch.ones(T, T))\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
        "torch.allclose(xbow, xbow2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhdOAd6-wXkZ",
        "outputId": "cbd015d1-0aa9-4fb5-917f-c06e7c1d494b"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# version 3: use Softmax\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "xbow3 = wei @ x\n",
        "torch.allclose(xbow, xbow3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOURrfG-ysoL",
        "outputId": "64b1fd4a-7b58-4db1-bc3e-bb8368db97e4"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# version 4: self-attention!\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "# let's see a single Head perform self-attention\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "k = key(x)   # (B, T, 16)\n",
        "q = query(x) # (B, T, 16)\n",
        "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "#wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "#out = wei @ x\n",
        "\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDarxEWIRMKq",
        "outputId": "c8094a05-ca7d-4d00-99fe-60fb8f70c24e"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vT1hdtzXCjgL",
        "outputId": "37cefdbf-406c-43d0-bdba-317bc3d8dc5f"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
              "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
              "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notes:\n",
        "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
        "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
        "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
        "- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
        "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
        "- \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
      ],
      "metadata": {
        "id": "M5CvobiQ0pLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k = torch.randn(B,T,head_size)\n",
        "q = torch.randn(B,T,head_size)\n",
        "wei = q @ k.transpose(-2, -1) * head_size**-0.5"
      ],
      "metadata": {
        "id": "4SNbLq5z3oBw"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nl6I9n9IRTSo",
        "outputId": "976fc21f-3dac-44e2-c25d-db527c2e84d1"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0449)"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1tQx7oeRvtc",
        "outputId": "1735c41e-997c-4a3c-c263-af38f7bc7987"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0700)"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLb_odHU3iKM",
        "outputId": "9a91d0af-edf1-4516-8276-4ce6e85c0178"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0918)"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JB82yzt44REI",
        "outputId": "b423f868-facd-48a5-f21c-59e9be1ddd9a"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mpt8569BB9_f",
        "outputId": "7dda5bec-d444-4886-f9b9-97c9324d64cb"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm1d: # (used to be BatchNorm1d)\n",
        "\n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    self.eps = eps\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # calculate the forward pass\n",
        "    xmean = x.mean(1, keepdim=True) # batch mean\n",
        "    xvar = x.var(1, keepdim=True) # batch variance\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "module = LayerNorm1d(100)\n",
        "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
        "x = module(x)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Num7sX9CKOH",
        "outputId": "e4f9a592-8b94-4f3e-9e67-3ec97918ee13"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[:,0].mean(), x[:,0].std() # mean,std of one feature across all batch inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "633T2cmnW1uk",
        "outputId": "e2f8c17c-0ee6-47e7-949b-1b7cd8e94f06"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.1469), tensor(0.8803))"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[0,:].mean(), x[0,:].std() # mean,std of a single input from the batch, of its features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LN9cK9BoXCYb",
        "outputId": "a841e74c-cb35-461f-87bf-2f7b37d8ae13"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(-9.5367e-09), tensor(1.0000))"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# French to English translation example:\n",
        "\n",
        "# <--------- ENCODE ------------------><--------------- DECODE ----------------->\n",
        "# les réseaux de neurones sont géniaux! <START> neural networks are awesome!<END>\n",
        "\n"
      ],
      "metadata": {
        "id": "dRJH6wM_XFfU"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Full finished code, for reference\n",
        "\n",
        "You may want to refer directly to the git repo instead though."
      ],
      "metadata": {
        "id": "ZcvKeBXoZFOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "#dataBSLLM.txt\n",
        "with open('data_new.txt', 'r', encoding='mac_roman') as f:\n",
        "  #CP1252\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "chars = chars[:-15]\n",
        "#chars = chars[:-11]\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "filtered_text = ''.join([c for c in text if c in stoi])\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(filtered_text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoelkOrFY8bN",
        "outputId": "52ca8192-aa2e-4b56-a92e-7198a166ccfc"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.212567 M parameters\n",
            "step 0: train loss 4.6202, val loss 4.6268\n",
            "step 100: train loss 2.6356, val loss 2.5813\n",
            "step 200: train loss 2.5127, val loss 2.4671\n",
            "step 300: train loss 2.4260, val loss 2.3952\n",
            "step 400: train loss 2.3506, val loss 2.3211\n",
            "step 500: train loss 2.2942, val loss 2.2511\n",
            "step 600: train loss 2.2481, val loss 2.2053\n",
            "step 700: train loss 2.1969, val loss 2.1469\n",
            "step 800: train loss 2.1547, val loss 2.1061\n",
            "step 900: train loss 2.0990, val loss 2.0682\n",
            "step 1000: train loss 2.0800, val loss 2.0290\n",
            "step 1100: train loss 2.0520, val loss 2.0049\n",
            "step 1200: train loss 2.0343, val loss 1.9689\n",
            "step 1300: train loss 2.0119, val loss 1.9352\n",
            "step 1400: train loss 1.9697, val loss 1.9236\n",
            "step 1500: train loss 1.9580, val loss 1.9004\n",
            "step 1600: train loss 1.9427, val loss 1.8863\n",
            "step 1700: train loss 1.9312, val loss 1.8577\n",
            "step 1800: train loss 1.9035, val loss 1.8553\n",
            "step 1900: train loss 1.8876, val loss 1.8245\n",
            "step 2000: train loss 1.8809, val loss 1.8182\n",
            "step 2100: train loss 1.8644, val loss 1.7909\n",
            "step 2200: train loss 1.8657, val loss 1.7845\n",
            "step 2300: train loss 1.8418, val loss 1.7828\n",
            "step 2400: train loss 1.8319, val loss 1.7620\n",
            "step 2500: train loss 1.8325, val loss 1.7580\n",
            "step 2600: train loss 1.8146, val loss 1.7394\n",
            "step 2700: train loss 1.8024, val loss 1.7186\n",
            "step 2800: train loss 1.7890, val loss 1.7144\n",
            "step 2900: train loss 1.7831, val loss 1.7050\n",
            "step 3000: train loss 1.7679, val loss 1.6872\n",
            "step 3100: train loss 1.7702, val loss 1.6828\n",
            "step 3200: train loss 1.7574, val loss 1.6856\n",
            "step 3300: train loss 1.7507, val loss 1.6757\n",
            "step 3400: train loss 1.7558, val loss 1.6767\n",
            "step 3500: train loss 1.7398, val loss 1.6562\n",
            "step 3600: train loss 1.7413, val loss 1.6395\n",
            "step 3700: train loss 1.7248, val loss 1.6548\n",
            "step 3800: train loss 1.7260, val loss 1.6496\n",
            "step 3900: train loss 1.7204, val loss 1.6476\n",
            "step 4000: train loss 1.7199, val loss 1.6309\n",
            "step 4100: train loss 1.7121, val loss 1.6290\n",
            "step 4200: train loss 1.7025, val loss 1.6275\n",
            "step 4300: train loss 1.7070, val loss 1.6236\n",
            "step 4400: train loss 1.6909, val loss 1.6061\n",
            "step 4500: train loss 1.6850, val loss 1.5967\n",
            "step 4600: train loss 1.6866, val loss 1.6112\n",
            "step 4700: train loss 1.6763, val loss 1.5925\n",
            "step 4800: train loss 1.6743, val loss 1.5915\n",
            "step 4900: train loss 1.6833, val loss 1.5937\n",
            "step 4999: train loss 1.6712, val loss 1.5873\n",
            "\n",
            "Elo, has howandarked up that would in the poires felt little sintranging. the constery weapped. He Misass are they seeked hes waswork theman his poshand. The Imn usain. Vin asked, not, leaved of bin it. Helkived on That Lord Ruxing, he wasnt what he was it curing six them grate. That heself to she Stell, save somethinglely working fring them. \n",
            "Ont have were \n",
            "Perhaps. Vin probe do what was what ce fell, are whimsting in her, theyre bilgg eight, I hour was strong on the perhsic of the his hus mascink works? \n",
            "Untotheirts,theysoge humiled pertbaherofforageofalmostan,anddhetatualingtheresbandandthingthemcordmedilefromthere.Istailing theristhedonlo.gincedgirneer. Inflercturingtheybusticaled spirtrablerevent. \n",
            "**Elend shooking. \n",
            "She lovy linger? What shopped busking and head hasnt bribly, need he ilfort; He seemed innow. \n",
            "Suildon. \n",
            "Bellings at will ners baffice how the pilates of metal back of our might. Talmony he set of a straded to to vish uncemes, finate reminated bettle seur, could, walking to the rack, must Elend the aff almous bleading in than not hard is hen string.ened shirp young the Mistrale sincle semilight, Waxilliung tom her bewtincing to6der? \n",
            "Inse entroved is the excarridass short. He sced. Thelle snad revleing a life briker, me that reads thagking. Brishand, and the Lord EmanRule but what Kelsiersonss. Yous use to smile,, laying the Elends overs. Youves in afound the short these dire blinking armonys before? Vin her canting the deal. Ond metal over. He never Tinding, their their on, the was a few long, his kneating. \n",
            "Arry \n",
            "*ous! Stels. \n",
            "Ham, Shere thought me embrapped bed the beganince all, normazer warxed. \n",
            "Waxtasate yous, need feeling of whispered breath I thought bean and mass \n",
            "Presecing the the wedis at I coulder the were, maneres dont mouth? Perhaps why where was? he reeps. \n",
            "Lord swound, and The standing its Fiss skanding probared over the palconted, at you thet very stilled, Vin give we reached back. She was dadoss. Well with spery. At Punshed. \n",
            "Izar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fjjvMifYZf7x"
      },
      "execution_count": 88,
      "outputs": []
    }
  ]
}